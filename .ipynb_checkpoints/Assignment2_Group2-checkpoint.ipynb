{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Members of Group B\n",
    "- Dominik Gulacsy\n",
    "- Anthony Njeru\n",
    "- Ruiqing Zhu\n",
    "\n",
    "### Link to Solution in Github Repository: \n",
    "https://github.com/dgulacsy/Assignment_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "\n",
    "The files in the folders 105‐extracted‐date and 106‐extracted‐date contain all speeches by U.S. senators in the 105th and 106th Congress (1997‐2000). The name of each file shows the congress‐name‐state abbreviation. For example, the file \"105‐akaka‐hi.txt\" contains all speeches by Senator Akaka from Hawaii in the 105th Congress (1997‐1998).\n",
    "\n",
    "The task is to count the frequency of words used by each senator in each Congress. For\n",
    "example, the number of times in the 105th Congress that senator Akaka mentioned the\n",
    "word \"gun\".\n",
    "\n",
    "1. Write a program that loops over directories and all files in the directories and prints the\n",
    "full file name.\n",
    "\n",
    "2. In the loop, read the speech files and split the text based on blanks (space) into words\n",
    "that are put in a list.\n",
    "\n",
    "3. Loop over words in the list and remove non‐alphanumeric characters, replace upper by\n",
    "lower case letters. \n",
    "\n",
    "4. Use the document droplist.txt  and check if the words in this list are also included in the stopword list provided by the NLTK library (you can find this in the inputs folder stopwords.txt). Join both lists in a list called stopwords_final and drop these words from each list in 3.\n",
    "\n",
    "5. Count the frequency of each remaining word and save the result in a comma‐separated file in \"long\" format where the first row contains the variable names: \"file\", \"word\", \"frequency\" and the following rows contain the corresponding values and save the file in the Output folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of target directories\n",
    "target_dirs=['105-extracted-date', '106-extracted-date']\n",
    "\n",
    "#Loop through target directories and file names to print them\n",
    "for target_dir in target_dirs:\n",
    "    for f_name in os.listdir(target_dir):\n",
    "        print(f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4\n",
    "with open('nltk_stopwords.txt','r') as f:\n",
    "    stopwords=f.read().splitlines()\n",
    "stopwords\n",
    "\n",
    "with open('droplist.txt','r') as f:\n",
    "    dropwords=re.sub('\"','',f.read()).split(\"\\n\")\n",
    "\n",
    "# initalize counter\n",
    "c=0\n",
    "\n",
    "# check for every word in dropwords if it is also in stopwords\n",
    "for e in dropwords:\n",
    "    if e not in stopwords:\n",
    "        c=c+1\n",
    "\n",
    "if c==0 :\n",
    "    print(\"Words in droplist.txt is a subset of words in stopwords.txt\")\n",
    "\n",
    "else:\n",
    "    print(\"Words in droplist.txt is NOT a subset of words in stopwords.txt\")\n",
    "    \n",
    "    # Unite the two lists (add new words from stopwords to dropwords)\n",
    "    for e in stopwords:\n",
    "        if e not in dropwords:\n",
    "            dropwords.append(e)\n",
    "            \n",
    "stopwords_final=dropwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5\n",
    "with open('Senators_Word_Frequencies.csv','w') as f:\n",
    "    f.write(\"file\"+','+\"word\"+','+\"frequency\"+\"\\n\")\n",
    "\n",
    "# Read in speech files\n",
    "all_sen_speeches = []\n",
    "for target_dir in target_dirs:\n",
    "    for f_name in os.listdir(target_dir):\n",
    "        with open(target_dir + '/' + f_name,'r') as f:\n",
    "            print(\"Reading in: \",f_name)\n",
    "            \n",
    "            # Remove new line symbols to make Regexp matching easier\n",
    "            file=re.sub(\"\\n\",\"\",f.read())\n",
    "\n",
    "        # Use the <TEXT> tags to search for speech text body.\n",
    "        pattern = re.compile(r\"<TEXT>(.*?)<\\/TEXT>\")\n",
    "        speeches = re.findall(pattern,file)\n",
    "        print('First speech is: ',speeches[0])\n",
    "\n",
    "        # Join senator's speeches and split them using whitespaces\n",
    "        speeches = ' '.join(speeches).split()\n",
    "        print('First word of speech is: ',speeches[0])\n",
    "\n",
    "        # Task 2\n",
    "        #Remove all non-alphanumerics and convert all to lowercase\n",
    "        speeches = [re.sub('\\W','',word.lower()) for word in speeches]\n",
    "        print('First 4 cleaned words of speeches are: ',speeches[:4])\n",
    "\n",
    "        # Task 4\n",
    "        # Remove words in stopwords_final from all_sen_speeches\n",
    "        for word in speeches:\n",
    "            if word in stopwords_final:\n",
    "                print('Status: ',speeches.index(word),\"/\",len(speeches))\n",
    "                print('Dropping the word: ',word,speeches[speeches.index(word)])\n",
    "                speeches.pop(speeches.index(word))\n",
    "\n",
    "\n",
    "        print(\"End of stopwords deletion process\")\n",
    "        # Task 5\n",
    "        word_freq = Counter(speeches)\n",
    "        with open('Senators_Word_Frequencies.csv','a') as f:\n",
    "            for word, freq in Counter(word_freq).items():\n",
    "                f.write(f_name + ',' + str(word) + ',' + str(freq)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Senators_Word_Frequencies.csv','w') as f:\n",
    "    f.write(\"file\"+','+\"word\"+','+\"frequency\"+\"\\n\")\n",
    "\n",
    "f_name='105-extracted-date/105-abraham-mi.txt'\n",
    "with open('105-extracted-date/105-abraham-mi.txt','r') as f:\n",
    "    print(\"Reading in: \",f_name)\n",
    "\n",
    "    # Remove new line symbols to make Regexp matching easier\n",
    "    file=re.sub(\"\\n\",\"\",f.read())\n",
    "\n",
    "# Use the <TEXT> tags to search for speech text body.\n",
    "pattern = re.compile(r\"<TEXT>(.*?)<\\/TEXT>\")\n",
    "speeches = re.findall(pattern,file)\n",
    "print('First speech is: ',speeches[0])\n",
    "\n",
    "# Join senator's speeches and split them using whitespaces\n",
    "speeches = ' '.join(speeches).split()\n",
    "print('First word of speech is: ',speeches[0])\n",
    "\n",
    "# Task 2\n",
    "#Remove all non-alphanumerics and convert all to lowercase\n",
    "speeches = [re.sub('\\W','',word.lower()) for word in speeches]\n",
    "print('First 4 cleaned words of speeches are: ',speeches[:4])\n",
    "\n",
    "# Task 4\n",
    "# Remove words in stopwords_final from all_sen_speeches\n",
    "for word in speeches:\n",
    "    if word in stopwords_final:\n",
    "        print('Status: ',speeches.index(word),\"/\",len(speeches))\n",
    "        speeches.pop(speeches.index(word))\n",
    "        print('Dropped the word: ',word,speeches[speeches.index(word)])\n",
    "\n",
    "print(\"End of deleting stopwords\")\n",
    "word_freq = Counter(speeches)\n",
    "with open('Senators_Word_Frequencies.csv','a') as f:\n",
    "    for word, freq in Counter(word_freq).items():\n",
    "        f.write(f_name + ',' + str(word) + ',' + str(freq)+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
